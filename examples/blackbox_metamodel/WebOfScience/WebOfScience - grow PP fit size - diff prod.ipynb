{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "757faaca",
   "metadata": {},
   "source": [
    "# Grow Performance Predictor (PP) fit size then predict with multiple prods\n",
    "\n",
    "In this notebook, we will do the following\n",
    "  - Start with the WebOfScience dataset\n",
    "  - Use the 90 classes with the highest class accuracy size\n",
    "    - 90 classes deteremined in notebook: \n",
    "      - 'WebOfScience - filter accuracy ordered classes to 0.80 acc and 90 classes.ipynb'\n",
    "  - Randomize the dataset.\n",
    "    - Train set is first 4,500 examples in randomized dataset. \n",
    "      - Mimics the size of the representative workspace\n",
    "    - Remaining 29,120 can be used to fit PP.\n",
    "  - Create base SVC model with the train set.\n",
    "  - Run the basic PP ShortTextClassificationWrapper varying the PP fit size.\n",
    "  - Start with 500 and then double the size.\n",
    "    - 500, 1000, 2000, 4000, 8000, 16000, 29120 (remainder)\n",
    "  - save the y_pred and y_score with the prod and log examples.\n",
    "  - Display the SVC and PP accuracies for SVC and each PP run\n",
    "\n",
    "Split remaining dataset into multiple prods to see the variation of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0a022d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-10 22:06:28.367390: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-08-10 22:06:28.367421: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gzip\n",
    "from IPython.display import display, HTML\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "from typing import List\n",
    "\n",
    "from uq360.algorithms.blackbox_metamodel.short_text_classification import ShortTextClassificationWrapper\n",
    "\n",
    "os.environ[\"PYTHONWARNINGS\"] = 'ignore'\n",
    "\n",
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Increase the width of the notebook so that it is the width of the browser \n",
    "# which allows larger size for the dashboard\n",
    "display(HTML('<style>.container { width:100% !important; }</style>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e2c6cb",
   "metadata": {},
   "source": [
    "#### Load workspace dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb390fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intent</th>\n",
       "      <th>n unique</th>\n",
       "      <th>min n uniq</th>\n",
       "      <th>max n uniq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>y</td>\n",
       "      <td>134</td>\n",
       "      <td>43</td>\n",
       "      <td>750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yl1</td>\n",
       "      <td>7</td>\n",
       "      <td>3297</td>\n",
       "      <td>14625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_merge.shape = (46985, 2)\n",
      "x.shape        = (46985,)\n",
      "y.shape        = (46985,)\n",
      "CPU times: user 213 ms, sys: 65.8 ms, total: 279 ms\n",
      "Wall time: 296 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# X is input data that include text sequences \n",
    "# Y is target value \n",
    "# YL1 is target value of level one (parent label)\n",
    "# YL2 is target value of level one (child label)\n",
    "x_gzip_file = '../../../data/WebOfScience/WebOfScience/WOS46985/X.txt.gzip'\n",
    "y_file = '../../../data/WebOfScience/WebOfScience/WOS46985/Y.txt'\n",
    "yl1_file = '../../../data/WebOfScience/WebOfScience/WOS46985/YL1.txt'\n",
    "yl2_file = '../../../data/WebOfScience/WebOfScience/WOS46985/YL2.txt'\n",
    "\n",
    "with gzip.open(x_gzip_file, 'rt') as f:\n",
    "    lines = f.readlines()\n",
    "df_x = pd.DataFrame(lines, columns=['example'])\n",
    "df_y = pd.read_csv(y_file, header=None, names=['intent'])\n",
    "df_yl1 = pd.read_csv(yl1_file, header=None, names=['yl1'])\n",
    "df_yl2 = pd.read_csv(yl2_file, header=None, names=['yl2'])\n",
    "data = [{'intent': 'y', \n",
    "         'n unique': len(np.unique(df_y['intent'])),\n",
    "         'min n uniq': min(np.unique(df_y['intent'], return_counts=True)[1]),\n",
    "         'max n uniq': max(np.unique(df_y['intent'], return_counts=True)[1])\n",
    "        },\n",
    "        {'intent': 'yl1', \n",
    "         'n unique': len(np.unique(df_yl1['yl1'])),\n",
    "         'min n uniq': min(np.unique(df_yl1['yl1'], return_counts=True)[1]),\n",
    "         'max n uniq': max(np.unique(df_yl1['yl1'], return_counts=True)[1])\n",
    "        }\n",
    "]\n",
    "display(HTML(pd.DataFrame(data).to_html()))\n",
    "\n",
    "df_merge = pd.concat([df_x, df_y], axis=1, sort=False)\n",
    "print(f'df_merge.shape = {df_merge.shape}')\n",
    "\n",
    "x = df_merge['example'].to_numpy()\n",
    "y = df_merge['intent'].to_numpy().ravel()\n",
    "print(f'x.shape        = {x.shape}')\n",
    "print(f'y.shape        = {y.shape}')\n",
    "\n",
    "# display(HTML(df_merge.head(4).to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580d651a",
   "metadata": {},
   "source": [
    "#### Encode with USE encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f82bf3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_encoded.shape = (46985, 384)\n",
      "CPU times: user 4.42 s, sys: 305 ms, total: 4.72 s\n",
      "Wall time: 4.73 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "class MiniLMEmbedding:\n",
    "    def __init__(self):\n",
    "        self.transformer = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "    def encode(self, input_sentences: List[str]) -> np.array:\n",
    "        sentences = [sentence.lower() for sentence in input_sentences]\n",
    "        embedded_sentences = [self.embed_sentence(s) for s in sentences]\n",
    "        return np.array(embedded_sentences)\n",
    "    def embed_sentence(self, sentence: str) -> np.array:\n",
    "        embedding = self.transformer.encode(sentence, show_progress_bar=False, convert_to_numpy=True)\n",
    "        return embedding\n",
    "\n",
    "encoded_file = '../../../data/WebOfScience/WebOfScience/WOS46985/X_encoded.csv'\n",
    "if os.path.exists(encoded_file):\n",
    "    df = pd.read_csv(encoded_file, header=None)\n",
    "    x_encoded = df.to_numpy()\n",
    "else:\n",
    "    encoder = MiniLMEmbedding()\n",
    "    x_encoded = encoder.encode(x)\n",
    "    # Save to file\n",
    "    df = pd.DataFrame(x_encoded)\n",
    "    df.to_csv(encoded_file, header=False, index=False)\n",
    "\n",
    "print(f'x_encoded.shape = {x_encoded.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b835b4f",
   "metadata": {},
   "source": [
    "#### Determine subset datasert for running experiment\n",
    "- keep_intents is generated in notebook:\n",
    "  - 'WebOfScience - filter accuracy ordered classes to 0.80 acc and 90 classes.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85ff3604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(keep_intents) = 90\n",
      "keep_intents = [64, 122, 12, 113, 62, 49, 66, 2, 68, 45, 103, 97, 70, 48, 115, 98, 3, 57, 61, 8, 74, 47, 127, 112, 65, 31, 99, 9, 79, 114, 35, 63, 111, 94, 101, 92, 46, 100, 69, 93, 96, 42, 25, 60, 39, 106, 121, 44, 33, 109, 14, 130, 81, 53, 17, 58, 71, 132, 80, 0, 83, 37, 55, 90, 85, 32, 75, 105, 22, 38, 56, 41, 128, 5, 21, 84, 43, 54, 36, 77, 27, 131, 72, 73, 118, 7, 108, 23, 26, 124]\n",
      "len(keep_indices) = 33620\n",
      "y_sub.shape         = (33620,)\n",
      "x_sub_encoded.shape = (33620, 384)\n"
     ]
    }
   ],
   "source": [
    "keep_intents = [64, 122, 12, 113, 62, 49, 66, 2, 68, 45, 103, 97, 70, 48, 115, 98, 3, 57, 61, 8, 74, 47, 127, 112, 65, 31, 99, 9, 79, 114, 35, 63, 111, 94, 101, 92, 46, 100, 69, 93, 96, 42, 25, 60, 39, 106, 121, 44, 33, 109, 14, 130, 81, 53, 17, 58, 71, 132, 80, 0, 83, 37, 55, 90, 85, 32, 75, 105, 22, 38, 56, 41, 128, 5, 21, 84, 43, 54, 36, 77, 27, 131, 72, 73, 118, 7, 108, 23, 26, 124]\n",
    "print(f'len(keep_intents) = {len(keep_intents)}')\n",
    "print(f'keep_intents = {list(keep_intents)}')\n",
    "# Gather the example indices for the examples to keep\n",
    "keep_indices = [i for i in range(len(y)) if y[i] in keep_intents]\n",
    "print(f'len(keep_indices) = {len(keep_indices)}')\n",
    "y_sub = y[keep_indices]\n",
    "x_sub_encoded = x_encoded[keep_indices]\n",
    "print(f'y_sub.shape         = {y_sub.shape}')\n",
    "print(f'x_sub_encoded.shape = {x_sub_encoded.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6f1f54",
   "metadata": {},
   "source": [
    "#### Randomize the dataset\n",
    "- Train set is first 4,500 examples in randomized dataset.\n",
    "  - Mimics the size of the representative workspace\n",
    "- Remaining 29,120 can be used to fit PP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c6c5489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_sub.shape         = (33620,)\n",
      "x_sub_encoded.shape = (33620, 384)\n",
      "y.shape             = (33620,)\n",
      "x.shape             = (33620, 384)\n",
      "y_train.shape = (4500,)\n",
      "x_train.shape = (4500, 384)\n",
      "y_test.shape  = (29120,)\n",
      "x_test.shape  = (29120, 384)\n"
     ]
    }
   ],
   "source": [
    "x, y = shuffle(x_sub_encoded, y_sub, random_state=42)\n",
    "print(f'y_sub.shape         = {y_sub.shape}')\n",
    "print(f'x_sub_encoded.shape = {x_sub_encoded.shape}')\n",
    "print(f'y.shape             = {y.shape}')\n",
    "print(f'x.shape             = {x.shape}')\n",
    "\n",
    "train_size = 4500\n",
    "# train_size = 500\n",
    "x_train = x[:train_size]\n",
    "y_train = y[:train_size]\n",
    "x_test = x[- (len(y) - train_size):]\n",
    "y_test = y[- (len(y) - train_size):]\n",
    "print(f'y_train.shape = {y_train.shape}')\n",
    "print(f'x_train.shape = {x_train.shape}')\n",
    "print(f'y_test.shape  = {y_test.shape}')\n",
    "print(f'x_test.shape  = {x_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6359ad7",
   "metadata": {},
   "source": [
    "#### Fit a basic SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3725e357",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 57.3 s, sys: 200 ms, total: 57.5 s\n",
      "Wall time: 58.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def train_model_svm(x, y):\n",
    "    model = SVC(probability=True)\n",
    "    model.fit(x, y)    \n",
    "    return model\n",
    "\n",
    "model = train_model_svm(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc817b8",
   "metadata": {},
   "source": [
    "#### Fit the Performance Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fc23c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictor type : text_ensemble\n",
      "calibrator : isotonic_regression\n",
      "metamodels considered: {'svm': ['confidence_top', 'confidence_delta', 'confidence_entropy', 'class_frequency', 'mlp', 'svc', 'predicted_class', 'one_class_svm', 'pca'], 'gbm': ['confidence_top', 'confidence_delta', 'confidence_entropy', 'class_frequency', 'mlp', 'svc', 'predicted_class', 'one_class_svm', 'pca'], 'mlp': ['confidence_top', 'confidence_delta', 'confidence_entropy', 'class_frequency', 'mlp', 'svc', 'predicted_class', 'one_class_svm', 'pca']}\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "Balancing data encountered a problem. Using unbalanced data.\n",
      "Balancing data encountered a problem. Using unbalanced data.\n",
      "Balancing data encountered a problem. Using unbalanced data.\n",
      "500 dur=813.2612860202789 - pp_fitted\n",
      "500 test dur=1.6453895568847656 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 test dur=3.7160212993621826 - pp_accuracy  = 0.6188468670453267\n",
      "500 prod_1 dur=1.5382626056671143 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 prod_1 dur=3.623178482055664 - pp_accuracy  = 0.6413162611567939\n",
      "500 prod_2 dur=1.6023578643798828 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 prod_2 dur=3.781388998031616 - pp_accuracy  = 0.637888644584674\n",
      "500 prod_3 dur=1.6378071308135986 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 prod_3 dur=3.9429337978363037 - pp_accuracy  = 0.6396472079118197\n",
      "500 prod_4 dur=1.6001818180084229 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 prod_4 dur=3.9830310344696045 - pp_accuracy  = 0.6402779360819296\n",
      "500 prod_5 dur=1.9496791362762451 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 prod_5 dur=4.145884275436401 - pp_accuracy  = 0.6460134203478219\n",
      "500 prod_6 dur=2.037543296813965 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 prod_6 dur=4.29499363899231 - pp_accuracy  = 0.6440073114650418\n",
      "500 prod_7 dur=1.8606841564178467 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 prod_7 dur=4.785702228546143 - pp_accuracy  = 0.6410990129292741\n",
      "500 prod_8 dur=2.255967378616333 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 prod_8 dur=4.56358003616333 - pp_accuracy  = 0.6384688023313978\n",
      "500 prod_9 dur=3.1463727951049805 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 prod_9 dur=4.213020324707031 - pp_accuracy  = 0.6428973866091229\n",
      "500 prod_10 dur=1.8995838165283203 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 prod_10 dur=3.9774117469787598 - pp_accuracy  = 0.6411255423180928\n",
      "500 prod_11 dur=1.7565336227416992 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 prod_11 dur=4.015035629272461 - pp_accuracy  = 0.6431791114932055\n",
      "500 prod_12 dur=1.636497974395752 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 prod_12 dur=3.759700059890747 - pp_accuracy  = 0.6322032768423255\n",
      "500 prod_13 dur=1.6693215370178223 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 prod_13 dur=3.7856783866882324 - pp_accuracy  = 0.6395936512893082\n",
      "500 prod_14 dur=1.722919225692749 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 prod_14 dur=3.8246395587921143 - pp_accuracy  = 0.6432421442469264\n",
      "500 prod_15 dur=1.6529674530029297 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 prod_15 dur=3.824967622756958 - pp_accuracy  = 0.6364343533451159\n",
      "500 prod_16 dur=1.6460614204406738 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 prod_16 dur=3.8323488235473633 - pp_accuracy  = 0.6425762807577395\n",
      "500 prod_17 dur=1.6714892387390137 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 prod_17 dur=3.8756752014160156 - pp_accuracy  = 0.6332552760754523\n",
      "500 prod_18 dur=1.7089200019836426 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 prod_18 dur=3.9819633960723877 - pp_accuracy  = 0.6416186049540058\n",
      "500 prod_19 dur=1.7090225219726562 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 prod_19 dur=4.48860502243042 - pp_accuracy  = 0.6411426214681595\n",
      "500 prod_20 dur=2.0042781829833984 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 prod_20 dur=4.967599153518677 - pp_accuracy  = 0.6411407918592874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 prod_21 dur=1.8637526035308838 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 prod_21 dur=4.766974449157715 - pp_accuracy  = 0.6386710523988126\n",
      "500 prod_22 dur=2.213740348815918 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 prod_22 dur=5.598948955535889 - pp_accuracy  = 0.6355373769950535\n",
      "500 prod_23 dur=2.157313346862793 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 prod_23 dur=5.360239267349243 - pp_accuracy  = 0.6388826190834295\n",
      "500 prod_24 dur=2.288783550262451 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 prod_24 dur=5.5933616161346436 - pp_accuracy  = 0.6354552166738867\n",
      "500 prod_25 dur=2.455500841140747 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 prod_25 dur=5.598010063171387 - pp_accuracy  = 0.6386258507442191\n",
      "500 prod_26 dur=2.325857639312744 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 prod_26 dur=5.289848327636719 - pp_accuracy  = 0.6312610081756849\n",
      "500 prod_27 dur=2.275467872619629 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 prod_27 dur=4.572070598602295 - pp_accuracy  = 0.6409804060412932\n",
      "500 prod_28 dur=1.9999980926513672 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 prod_28 dur=4.2674174308776855 - pp_accuracy  = 0.6494321225930713\n",
      "500 prod_29 dur=1.9696753025054932 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 prod_29 dur=5.470164775848389 - pp_accuracy  = 0.6341691040715546\n",
      "500 prod_30 dur=2.2363924980163574 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 prod_30 dur=5.309542179107666 - pp_accuracy  = 0.6363772081643837\n",
      "500 prod_31 dur=2.247896671295166 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 prod_31 dur=5.352694511413574 - pp_accuracy  = 0.6406683749131338\n",
      "500 prod_32 dur=2.2819037437438965 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 prod_32 dur=5.335665225982666 - pp_accuracy  = 0.6393306306843511\n",
      "500 prod_33 dur=2.183595657348633 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 prod_33 dur=4.657848596572876 - pp_accuracy  = 0.6372949571661294\n",
      "500 prod_34 dur=1.8779914379119873 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 prod_34 dur=5.17690896987915 - pp_accuracy  = 0.6411780891813159\n",
      "500 prod_35 dur=2.04760479927063 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 prod_35 dur=4.284874439239502 - pp_accuracy  = 0.6346234347165463\n",
      "500 prod_36 dur=2.006394147872925 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 prod_36 dur=4.4430859088897705 - pp_accuracy  = 0.6356599896250631\n",
      "500 prod_37 dur=2.6837968826293945 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 prod_37 dur=4.058790922164917 - pp_accuracy  = 0.6366965155300383\n",
      "500 prod_38 dur=1.8301606178283691 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 prod_38 dur=5.911580801010132 - pp_accuracy  = 0.6491925111973467\n",
      "500 prod_39 dur=2.491748809814453 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 prod_39 dur=4.667689800262451 - pp_accuracy  = 0.6436562103808319\n",
      "500 prod_40 dur=1.5262575149536133 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 prod_40 dur=3.6996147632598877 - pp_accuracy  = 0.6395813967098667\n",
      "500 prod_41 dur=1.5642614364624023 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 prod_41 dur=4.926624536514282 - pp_accuracy  = 0.6415236979225033\n",
      "500 prod_42 dur=2.1860568523406982 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 prod_42 dur=4.481415271759033 - pp_accuracy  = 0.6368003343800397\n",
      "500 prod_43 dur=1.5650286674499512 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 prod_43 dur=3.6314735412597656 - pp_accuracy  = 0.6405639982143855\n",
      "500 prod_44 dur=1.6191766262054443 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 prod_44 dur=3.878439426422119 - pp_accuracy  = 0.638427578597065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 prod_45 dur=1.6514496803283691 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 prod_45 dur=5.016313791275024 - pp_accuracy  = 0.6403518611776324\n",
      "500 prod_46 dur=2.4154398441314697 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 prod_46 dur=4.702132225036621 - pp_accuracy  = 0.6319919330441792\n",
      "500 prod_47 dur=1.5062661170959473 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 prod_47 dur=3.8923346996307373 - pp_accuracy  = 0.6413688634411733\n",
      "500 prod_48 dur=1.668346881866455 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 prod_48 dur=5.326167345046997 - pp_accuracy  = 0.6389315000053131\n",
      "500 prod_49 dur=2.486565589904785 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 prod_49 dur=5.487216234207153 - pp_accuracy  = 0.6420793715966883\n",
      "500 prod_50 dur=1.6644783020019531 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 prod_50 dur=3.9330697059631348 - pp_accuracy  = 0.6336648900038196\n",
      "500 prod_51 dur=1.6492588520050049 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 prod_51 dur=3.9326910972595215 - pp_accuracy  = 0.6427849485831255\n",
      "500 prod_52 dur=1.6484739780426025 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 prod_52 dur=3.8895652294158936 - pp_accuracy  = 0.6405453260416094\n",
      "500 prod_53 dur=1.5920765399932861 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 prod_53 dur=4.618968486785889 - pp_accuracy  = 0.647289848715363\n",
      "500 prod_54 dur=2.084958553314209 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 prod_54 dur=4.982105731964111 - pp_accuracy  = 0.6376291263047189\n",
      "500 prod_55 dur=2.079409599304199 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 prod_55 dur=4.108108282089233 - pp_accuracy  = 0.6445343686960564\n",
      "500 prod_56 dur=1.669774055480957 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 prod_56 dur=3.9372775554656982 - pp_accuracy  = 0.6412926944606105\n",
      "500 prod_57 dur=1.6280877590179443 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 prod_57 dur=3.883284091949463 - pp_accuracy  = 0.6393185935622053\n",
      "500 prod_58 dur=0.38975048065185547 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "500 prod_58 dur=1.1150200366973877 - pp_accuracy  = 0.6360050959875417\n",
      "Predictor type : text_ensemble\n",
      "calibrator : isotonic_regression\n",
      "metamodels considered: {'svm': ['confidence_top', 'confidence_delta', 'confidence_entropy', 'class_frequency', 'mlp', 'svc', 'predicted_class', 'one_class_svm', 'pca'], 'gbm': ['confidence_top', 'confidence_delta', 'confidence_entropy', 'class_frequency', 'mlp', 'svc', 'predicted_class', 'one_class_svm', 'pca'], 'mlp': ['confidence_top', 'confidence_delta', 'confidence_entropy', 'class_frequency', 'mlp', 'svc', 'predicted_class', 'one_class_svm', 'pca']}\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "Balancing data encountered a problem. Using unbalanced data.\n",
      "Balancing data encountered a problem. Using unbalanced data.\n",
      "Balancing data encountered a problem. Using unbalanced data.\n",
      "1000 dur=547.6596443653107 - pp_fitted\n",
      "1000 test dur=4.771432638168335 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "1000 test dur=9.411938667297363 - pp_accuracy  = 0.6229105496579517\n",
      "1000 prod_1 dur=3.615229368209839 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "1000 prod_1 dur=10.0486319065094 - pp_accuracy  = 0.6252149053931564\n",
      "1000 prod_2 dur=4.727149248123169 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "1000 prod_2 dur=10.551232814788818 - pp_accuracy  = 0.6316097100352289\n",
      "1000 prod_3 dur=4.53090500831604 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "1000 prod_3 dur=9.349838256835938 - pp_accuracy  = 0.6365843117006228\n",
      "1000 prod_4 dur=3.2968170642852783 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "1000 prod_4 dur=10.579078435897827 - pp_accuracy  = 0.628873587837057\n",
      "1000 prod_5 dur=3.5969955921173096 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "1000 prod_5 dur=10.131927490234375 - pp_accuracy  = 0.632661503036898\n",
      "1000 prod_6 dur=4.835768938064575 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "1000 prod_6 dur=8.125644207000732 - pp_accuracy  = 0.6196831005161193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 prod_7 dur=4.534213542938232 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "1000 prod_7 dur=10.410158395767212 - pp_accuracy  = 0.6256961589035206\n",
      "1000 prod_8 dur=3.397735118865967 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "1000 prod_8 dur=7.981476783752441 - pp_accuracy  = 0.6239158274827536\n",
      "1000 prod_9 dur=4.917804718017578 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "1000 prod_9 dur=10.065426111221313 - pp_accuracy  = 0.6339578766567852\n",
      "1000 prod_10 dur=3.3933329582214355 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "1000 prod_10 dur=9.714431285858154 - pp_accuracy  = 0.6259691689622046\n",
      "1000 prod_11 dur=3.9368512630462646 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "1000 prod_11 dur=7.310400485992432 - pp_accuracy  = 0.6215399785265172\n",
      "1000 prod_12 dur=3.1165871620178223 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "1000 prod_12 dur=8.054959058761597 - pp_accuracy  = 0.6246816795802054\n",
      "1000 prod_13 dur=4.095218896865845 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "1000 prod_13 dur=9.608892440795898 - pp_accuracy  = 0.6152308329829312\n",
      "1000 prod_14 dur=3.238570213317871 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "1000 prod_14 dur=7.971685886383057 - pp_accuracy  = 0.6313920795444791\n",
      "1000 prod_15 dur=3.5673043727874756 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "1000 prod_15 dur=11.579196691513062 - pp_accuracy  = 0.6234659454115894\n",
      "1000 prod_16 dur=3.484104633331299 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "1000 prod_16 dur=8.051334142684937 - pp_accuracy  = 0.6245484725928171\n",
      "1000 prod_17 dur=3.4237542152404785 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "1000 prod_17 dur=11.679875135421753 - pp_accuracy  = 0.6235120880946087\n",
      "1000 prod_18 dur=3.366011619567871 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "1000 prod_18 dur=7.857722282409668 - pp_accuracy  = 0.6153062415029769\n",
      "1000 prod_19 dur=3.390847682952881 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "1000 prod_19 dur=7.9844489097595215 - pp_accuracy  = 0.6394890769643663\n",
      "1000 prod_20 dur=4.440573692321777 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "1000 prod_20 dur=9.626720428466797 - pp_accuracy  = 0.6284675653901476\n",
      "1000 prod_21 dur=3.373929262161255 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "1000 prod_21 dur=8.303828954696655 - pp_accuracy  = 0.619380048836342\n",
      "1000 prod_22 dur=5.269153833389282 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "1000 prod_22 dur=9.655911684036255 - pp_accuracy  = 0.6256378945221018\n",
      "1000 prod_23 dur=3.5310425758361816 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "1000 prod_23 dur=8.146706581115723 - pp_accuracy  = 0.6198675193329678\n",
      "1000 prod_24 dur=3.5145864486694336 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "1000 prod_24 dur=9.723105192184448 - pp_accuracy  = 0.6278535359794295\n",
      "1000 prod_25 dur=4.131426811218262 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "1000 prod_25 dur=7.896660566329956 - pp_accuracy  = 0.622315377824757\n",
      "1000 prod_26 dur=3.327573776245117 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "1000 prod_26 dur=7.515627861022949 - pp_accuracy  = 0.6307520194668895\n",
      "1000 prod_27 dur=3.118928909301758 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "1000 prod_27 dur=7.39218282699585 - pp_accuracy  = 0.6267673111221836\n",
      "1000 prod_28 dur=3.2289068698883057 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "1000 prod_28 dur=7.580167055130005 - pp_accuracy  = 0.628289576069943\n",
      "1000 prod_29 dur=0.4032120704650879 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "1000 prod_29 dur=0.9472098350524902 - pp_accuracy  = 0.6021827389723725\n",
      "Predictor type : text_ensemble\n",
      "calibrator : isotonic_regression\n",
      "metamodels considered: {'svm': ['confidence_top', 'confidence_delta', 'confidence_entropy', 'class_frequency', 'mlp', 'svc', 'predicted_class', 'one_class_svm', 'pca'], 'gbm': ['confidence_top', 'confidence_delta', 'confidence_entropy', 'class_frequency', 'mlp', 'svc', 'predicted_class', 'one_class_svm', 'pca'], 'mlp': ['confidence_top', 'confidence_delta', 'confidence_entropy', 'class_frequency', 'mlp', 'svc', 'predicted_class', 'one_class_svm', 'pca']}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "Balancing data encountered a problem. Using unbalanced data.\n",
      "Balancing data encountered a problem. Using unbalanced data.\n",
      "Balancing data encountered a problem. Using unbalanced data.\n",
      "2000 dur=571.4198591709137 - pp_fitted\n",
      "2000 test dur=6.401398181915283 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "2000 test dur=14.604142904281616 - pp_accuracy  = 0.6206402060741439\n",
      "2000 prod_1 dur=6.239683389663696 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "2000 prod_1 dur=15.773473262786865 - pp_accuracy  = 0.6123435049997706\n",
      "2000 prod_2 dur=6.548400640487671 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "2000 prod_2 dur=15.424905776977539 - pp_accuracy  = 0.60842517759378\n",
      "2000 prod_3 dur=7.636447429656982 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "2000 prod_3 dur=19.57110023498535 - pp_accuracy  = 0.5993991380666359\n",
      "2000 prod_4 dur=8.71627926826477 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "2000 prod_4 dur=20.259751558303833 - pp_accuracy  = 0.6097666284197575\n",
      "2000 prod_5 dur=7.574808835983276 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "2000 prod_5 dur=19.404247760772705 - pp_accuracy  = 0.6005864970396422\n",
      "2000 prod_6 dur=9.206227540969849 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "2000 prod_6 dur=20.378973960876465 - pp_accuracy  = 0.6001982049995833\n",
      "2000 prod_7 dur=7.143635988235474 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "2000 prod_7 dur=18.453275203704834 - pp_accuracy  = 0.6019156776658567\n",
      "2000 prod_8 dur=8.056865215301514 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "2000 prod_8 dur=17.340901851654053 - pp_accuracy  = 0.5991965724085767\n",
      "2000 prod_9 dur=7.641831874847412 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "2000 prod_9 dur=16.325270175933838 - pp_accuracy  = 0.6031936204781106\n",
      "2000 prod_10 dur=8.18345332145691 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "2000 prod_10 dur=13.669564962387085 - pp_accuracy  = 0.6011808242084837\n",
      "2000 prod_11 dur=5.991258382797241 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "2000 prod_11 dur=17.938626527786255 - pp_accuracy  = 0.599377638164882\n",
      "2000 prod_12 dur=6.342920780181885 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "2000 prod_12 dur=18.925999402999878 - pp_accuracy  = 0.602969711301968\n",
      "2000 prod_13 dur=6.296828508377075 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "2000 prod_13 dur=18.704692363739014 - pp_accuracy  = 0.6060188300014295\n",
      "2000 prod_14 dur=3.494546890258789 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "2000 prod_14 dur=8.560603618621826 - pp_accuracy  = 0.6011531745000213\n",
      "Predictor type : text_ensemble\n",
      "calibrator : isotonic_regression\n",
      "metamodels considered: {'svm': ['confidence_top', 'confidence_delta', 'confidence_entropy', 'class_frequency', 'mlp', 'svc', 'predicted_class', 'one_class_svm', 'pca'], 'gbm': ['confidence_top', 'confidence_delta', 'confidence_entropy', 'class_frequency', 'mlp', 'svc', 'predicted_class', 'one_class_svm', 'pca'], 'mlp': ['confidence_top', 'confidence_delta', 'confidence_entropy', 'class_frequency', 'mlp', 'svc', 'predicted_class', 'one_class_svm', 'pca']}\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "Balancing data encountered a problem. Using unbalanced data.\n",
      "Balancing data encountered a problem. Using unbalanced data.\n",
      "Balancing data encountered a problem. Using unbalanced data.\n",
      "4000 dur=662.9353158473969 - pp_fitted\n",
      "4000 test dur=12.428609848022461 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "4000 test dur=30.004406213760376 - pp_accuracy  = 0.6241408902955756\n",
      "4000 prod_1 dur=12.657149076461792 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "4000 prod_1 dur=30.638111114501953 - pp_accuracy  = 0.6168657204622697\n",
      "4000 prod_2 dur=13.497867584228516 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "4000 prod_2 dur=31.91062045097351 - pp_accuracy  = 0.6189634851505804\n",
      "4000 prod_3 dur=13.098986148834229 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "4000 prod_3 dur=34.00204849243164 - pp_accuracy  = 0.6144284879963928\n",
      "4000 prod_4 dur=15.131629467010498 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000 prod_4 dur=38.32997918128967 - pp_accuracy  = 0.614950278850181\n",
      "4000 prod_5 dur=16.478713035583496 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "4000 prod_5 dur=36.393545389175415 - pp_accuracy  = 0.6152916113788877\n",
      "4000 prod_6 dur=15.172109603881836 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "4000 prod_6 dur=35.06008219718933 - pp_accuracy  = 0.6175965355736696\n",
      "4000 prod_7 dur=4.12351655960083 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "4000 prod_7 dur=11.500207901000977 - pp_accuracy  = 0.6171753757644618\n",
      "Predictor type : text_ensemble\n",
      "calibrator : isotonic_regression\n",
      "metamodels considered: {'svm': ['confidence_top', 'confidence_delta', 'confidence_entropy', 'class_frequency', 'mlp', 'svc', 'predicted_class', 'one_class_svm', 'pca'], 'gbm': ['confidence_top', 'confidence_delta', 'confidence_entropy', 'class_frequency', 'mlp', 'svc', 'predicted_class', 'one_class_svm', 'pca'], 'mlp': ['confidence_top', 'confidence_delta', 'confidence_entropy', 'class_frequency', 'mlp', 'svc', 'predicted_class', 'one_class_svm', 'pca']}\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "Balancing data encountered a problem. Using unbalanced data.\n",
      "Balancing data encountered a problem. Using unbalanced data.\n",
      "Balancing data encountered a problem. Using unbalanced data.\n",
      "8000 dur=874.1270141601562 - pp_fitted\n",
      "8000 test dur=24.95937943458557 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "8000 test dur=59.63154983520508 - pp_accuracy  = 0.6239572156419864\n",
      "8000 prod_1 dur=26.21715259552002 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "8000 prod_1 dur=61.143248081207275 - pp_accuracy  = 0.6225196450502555\n",
      "8000 prod_2 dur=25.50511884689331 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "8000 prod_2 dur=62.90070819854736 - pp_accuracy  = 0.6193936619378908\n",
      "8000 prod_3 dur=16.281583786010742 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "8000 prod_3 dur=37.67326593399048 - pp_accuracy  = 0.6237074156991358\n",
      "Predictor type : text_ensemble\n",
      "calibrator : isotonic_regression\n",
      "metamodels considered: {'svm': ['confidence_top', 'confidence_delta', 'confidence_entropy', 'class_frequency', 'mlp', 'svc', 'predicted_class', 'one_class_svm', 'pca'], 'gbm': ['confidence_top', 'confidence_delta', 'confidence_entropy', 'class_frequency', 'mlp', 'svc', 'predicted_class', 'one_class_svm', 'pca'], 'mlp': ['confidence_top', 'confidence_delta', 'confidence_entropy', 'class_frequency', 'mlp', 'svc', 'predicted_class', 'one_class_svm', 'pca']}\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "Balancing data encountered a problem. Using unbalanced data.\n",
      "Balancing data encountered a problem. Using unbalanced data.\n",
      "Balancing data encountered a problem. Using unbalanced data.\n",
      "16000 dur=1186.899836063385 - pp_fitted\n",
      "16000 test dur=40.767375230789185 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "16000 test dur=111.0137689113617 - pp_accuracy  = 0.6310976847444931\n",
      "16000 prod_1 dur=41.01998686790466 - svc_predict\n",
      "Incoming data is already encoded\n",
      "Features extracted for : ['class_frequency', 'confidence_delta', 'confidence_entropy', 'confidence_top', 'mlp_1', 'mlp_2', 'one_class_svm', 'pca_1', 'pca_2', 'predicted_class', 'svc_1', 'svc_2']\n",
      "16000 prod_1 dur=104.50006890296936 - pp_accuracy  = 0.6286837908113665\n",
      "CPU times: user 54min 43s, sys: 9min 57s, total: 1h 4min 40s\n",
      "Wall time: 1h 52min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pointwise_features=['confidence_top',\n",
    "                    'confidence_delta',\n",
    "                    'confidence_entropy',\n",
    "                    'class_frequency',\n",
    "                    'mlp',\n",
    "                    'svc',\n",
    "                    'predicted_class',\n",
    "                    'one_class_svm',\n",
    "                    'pca']\n",
    "calibrator = 'isotonic_regression'\n",
    "metamodels_considered = {'svm': pointwise_features,\n",
    "                         'gbm': pointwise_features,\n",
    "                         'mlp': pointwise_features}\n",
    "\n",
    "experiments = []\n",
    "for size in [500, 1000, 2000, 4000, 8000, 16000]:\n",
    "# for size in [100, 200]:\n",
    "    pp = ShortTextClassificationWrapper(base_model=model, calibrator=calibrator, metamodels_considered=metamodels_considered)\n",
    "    x_tst = x_test[:size]\n",
    "    y_tst = y_test[:size]\n",
    "\n",
    "    # fit PP\n",
    "    start = time.time()\n",
    "    pp.fit(x_train, y_train, x_tst, y_tst)\n",
    "    print(f'{size} dur={time.time() - start} - pp_fitted')\n",
    "\n",
    "    start = time.time()\n",
    "    svc_pred = model.predict(x_tst)\n",
    "    print(f'{size} test dur={time.time() - start} - svc_predict')\n",
    "\n",
    "    start = time.time()\n",
    "    pp_accuracy, _, pp_score = pp.predict(x_tst)\n",
    "    print(f'{size} test dur={time.time() - start} - pp_accuracy  = {pp_accuracy / 100}')\n",
    "\n",
    "    experiments.append({'dataset': 'test',\n",
    "                        'pp fit size': len(y_tst),\n",
    "                        'pp pred size': len(y_tst),\n",
    "                        'y': y_tst,\n",
    "                        'svc_pred': svc_pred,\n",
    "                        'pp_accuracy': pp_accuracy / 100,\n",
    "                        'pp_score': pp_score[0],\n",
    "                       })\n",
    "\n",
    "    for prod_run in range(1, 60):\n",
    "        x_prod = x_test[size * prod_run:size * (prod_run + 1)]\n",
    "        y_prod = y_test[size * prod_run:size * (prod_run + 1)]\n",
    "        start = time.time()\n",
    "        svc_pred = model.predict(x_prod)\n",
    "        print(f'{size} prod_{prod_run} dur={time.time() - start} - svc_predict')\n",
    "\n",
    "        start = time.time()\n",
    "        pp_accuracy, _, pp_score = pp.predict(x_prod)\n",
    "        print(f'{size} prod_{prod_run} dur={time.time() - start} - pp_accuracy  = {pp_accuracy / 100}')\n",
    "\n",
    "        experiments.append({'dataset': f'prod_{prod_run}',\n",
    "                            'pp fit size': len(y_tst),\n",
    "                            'pp pred size': len(y_prod),\n",
    "                            'y': y_prod,\n",
    "                            'svc_pred': svc_pred,\n",
    "                            'pp_accuracy': pp_accuracy / 100,\n",
    "                            'pp_score': pp_score[0],\n",
    "                           })\n",
    "        if size * (prod_run + 1) > len(y_test):\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bc220c",
   "metadata": {},
   "source": [
    "#### Summarize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "750dada7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_correctness(intents, y_predictions, y_scores, threshold=0.9):\n",
    "    n_correct = 0\n",
    "    true_high = 0\n",
    "    true_low = 0\n",
    "    false_high = 0\n",
    "    false_low = 0\n",
    "    n_y = len(intents)\n",
    "    for y, y_pred, y_score in zip(intents, y_predictions, y_scores):\n",
    "        if y == y_pred:\n",
    "            n_correct += 1\n",
    "            if y_score > threshold:\n",
    "                true_high += 1\n",
    "            else:\n",
    "                true_low += 1\n",
    "        else:\n",
    "            if y_score > threshold:\n",
    "                false_high += 1\n",
    "            else:\n",
    "                false_low += 1\n",
    "    acc = n_correct / n_y\n",
    "    th = true_high / n_y\n",
    "    tl = true_low / n_y\n",
    "    fh = false_high / n_y\n",
    "    fl = false_low / n_y\n",
    "\n",
    "    return acc, th, tl, fh, fl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef2a86fe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>pp fit size</th>\n",
       "      <th>svc accuracy</th>\n",
       "      <th>pp accuracy</th>\n",
       "      <th>TH</th>\n",
       "      <th>TL</th>\n",
       "      <th>FH</th>\n",
       "      <th>FL</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test</td>\n",
       "      <td>500</td>\n",
       "      <td>62.60%</td>\n",
       "      <td>61.88%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>62.60%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>37.40%</td>\n",
       "      <td>90.72%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>prod_1</td>\n",
       "      <td>500</td>\n",
       "      <td>62.60%</td>\n",
       "      <td>64.13%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>62.60%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>37.40%</td>\n",
       "      <td>73.72%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>prod_2</td>\n",
       "      <td>500</td>\n",
       "      <td>58.00%</td>\n",
       "      <td>63.79%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>58.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>42.00%</td>\n",
       "      <td>72.16%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>prod_3</td>\n",
       "      <td>500</td>\n",
       "      <td>59.60%</td>\n",
       "      <td>63.96%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>59.60%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>40.40%</td>\n",
       "      <td>70.48%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>prod_4</td>\n",
       "      <td>500</td>\n",
       "      <td>64.80%</td>\n",
       "      <td>64.03%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>64.80%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>35.20%</td>\n",
       "      <td>69.83%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>prod_5</td>\n",
       "      <td>500</td>\n",
       "      <td>62.40%</td>\n",
       "      <td>64.60%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>62.40%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>37.60%</td>\n",
       "      <td>74.63%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>prod_6</td>\n",
       "      <td>500</td>\n",
       "      <td>66.00%</td>\n",
       "      <td>64.40%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>66.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>34.00%</td>\n",
       "      <td>73.95%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>prod_7</td>\n",
       "      <td>500</td>\n",
       "      <td>61.20%</td>\n",
       "      <td>64.11%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>61.20%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>38.80%</td>\n",
       "      <td>70.63%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>prod_8</td>\n",
       "      <td>500</td>\n",
       "      <td>62.00%</td>\n",
       "      <td>63.85%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>62.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>38.00%</td>\n",
       "      <td>75.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>prod_9</td>\n",
       "      <td>500</td>\n",
       "      <td>64.80%</td>\n",
       "      <td>64.29%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>64.80%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>35.20%</td>\n",
       "      <td>73.84%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>prod_10</td>\n",
       "      <td>500</td>\n",
       "      <td>64.60%</td>\n",
       "      <td>64.11%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>64.60%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>35.40%</td>\n",
       "      <td>75.01%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>prod_11</td>\n",
       "      <td>500</td>\n",
       "      <td>63.20%</td>\n",
       "      <td>64.32%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>63.20%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>36.80%</td>\n",
       "      <td>71.81%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>prod_12</td>\n",
       "      <td>500</td>\n",
       "      <td>60.40%</td>\n",
       "      <td>63.22%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>60.40%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>39.60%</td>\n",
       "      <td>75.12%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>prod_13</td>\n",
       "      <td>500</td>\n",
       "      <td>61.20%</td>\n",
       "      <td>63.96%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>61.20%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>38.80%</td>\n",
       "      <td>75.99%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>prod_14</td>\n",
       "      <td>500</td>\n",
       "      <td>60.80%</td>\n",
       "      <td>64.32%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>60.80%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>39.20%</td>\n",
       "      <td>71.40%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>prod_15</td>\n",
       "      <td>500</td>\n",
       "      <td>63.40%</td>\n",
       "      <td>63.64%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>63.40%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>36.60%</td>\n",
       "      <td>76.96%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>prod_16</td>\n",
       "      <td>500</td>\n",
       "      <td>65.60%</td>\n",
       "      <td>64.26%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>65.60%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>34.40%</td>\n",
       "      <td>74.97%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>prod_17</td>\n",
       "      <td>500</td>\n",
       "      <td>59.00%</td>\n",
       "      <td>63.33%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>59.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>41.00%</td>\n",
       "      <td>76.75%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>prod_18</td>\n",
       "      <td>500</td>\n",
       "      <td>66.80%</td>\n",
       "      <td>64.16%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>66.80%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>33.20%</td>\n",
       "      <td>73.86%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>prod_19</td>\n",
       "      <td>500</td>\n",
       "      <td>62.80%</td>\n",
       "      <td>64.11%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>62.80%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>37.20%</td>\n",
       "      <td>69.67%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.figure()\n",
    "data = []\n",
    "for e in experiments:\n",
    "    y_true = [1 if y==p else 0 for y,p in zip(e['y'], e['svc_pred'])]\n",
    "#     fpr, tpr, thresholds = roc_curve(y_true, e['pp_score'], pos_label=1)\n",
    "    auc = roc_auc_score(y_true, e['pp_score'])\n",
    "#     plt.plot(fpr, tpr, label=f'{auc:.2%} - {e[\"pp fit size\"]} - {e[\"dataset\"]}')\n",
    "\n",
    "    acc, th, tl, fh, fl = check_correctness(e['y'],  e['svc_pred'],  e['pp_score'])\n",
    "    data.append({'dataset': e['dataset'],\n",
    "                 'pp fit size': e['pp fit size'],\n",
    "                 'svc accuracy': acc,\n",
    "                 'pp accuracy': e['pp_accuracy'],\n",
    "                 'TH': th,\n",
    "                 'TL': tl,\n",
    "                 'FH': fh,\n",
    "                 'FL': fl,\n",
    "                 'AUC': auc,\n",
    "    })\n",
    "\n",
    "pct_fmt = '{:,.2%}'.format\n",
    "fmt = {col: pct_fmt for col in ['svc accuracy', 'pp accuracy', 'TH', 'TL', 'FH', 'FL', 'AUC']}\n",
    "display(HTML(pd.DataFrame(data).head(20).to_html(formatters=fmt)))\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# plt.title('Receiver Operating Characteristic')\n",
    "# plt.legend(loc='lower right')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4988823f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(data).to_csv('WOS-growPPfitSize-diffProd.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "449ada89",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv('WOS-growPPfitSize-diffProd.csv')\n",
    "\n",
    "# pct_fmt = '{:,.2%}'.format\n",
    "# fmt = {col: pct_fmt for col in ['svc accuracy', 'pp accuracy', 'TH', 'TL', 'FH', 'FL', 'AUC']}\n",
    "# display(HTML(df.to_html(formatters=fmt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6413832",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>pp fit size</th>\n",
       "      <th>svc accuracy</th>\n",
       "      <th>pp accuracy</th>\n",
       "      <th>TH</th>\n",
       "      <th>TL</th>\n",
       "      <th>FH</th>\n",
       "      <th>FL</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test</td>\n",
       "      <td>500</td>\n",
       "      <td>0.62600</td>\n",
       "      <td>0.618847</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.626000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.374000</td>\n",
       "      <td>0.907212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.62600</td>\n",
       "      <td>0.622911</td>\n",
       "      <td>0.028000</td>\n",
       "      <td>0.598000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.374000</td>\n",
       "      <td>0.855594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.60700</td>\n",
       "      <td>0.620640</td>\n",
       "      <td>0.132500</td>\n",
       "      <td>0.474500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.393000</td>\n",
       "      <td>0.955779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test</td>\n",
       "      <td>4000</td>\n",
       "      <td>0.62150</td>\n",
       "      <td>0.624141</td>\n",
       "      <td>0.162750</td>\n",
       "      <td>0.458750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.378500</td>\n",
       "      <td>0.957341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test</td>\n",
       "      <td>8000</td>\n",
       "      <td>0.62350</td>\n",
       "      <td>0.623957</td>\n",
       "      <td>0.199250</td>\n",
       "      <td>0.424250</td>\n",
       "      <td>0.011750</td>\n",
       "      <td>0.364750</td>\n",
       "      <td>0.816682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>test</td>\n",
       "      <td>16000</td>\n",
       "      <td>0.62375</td>\n",
       "      <td>0.631098</td>\n",
       "      <td>0.193938</td>\n",
       "      <td>0.429812</td>\n",
       "      <td>0.014563</td>\n",
       "      <td>0.361687</td>\n",
       "      <td>0.805379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>pp fit size</th>\n",
       "      <th>svc accuracy</th>\n",
       "      <th>pp accuracy</th>\n",
       "      <th>TH</th>\n",
       "      <th>TL</th>\n",
       "      <th>FH</th>\n",
       "      <th>FL</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>prod_1</td>\n",
       "      <td>500</td>\n",
       "      <td>0.626</td>\n",
       "      <td>0.641316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.626</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.374</td>\n",
       "      <td>0.737173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>prod_2</td>\n",
       "      <td>500</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.637889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.721576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>prod_3</td>\n",
       "      <td>500</td>\n",
       "      <td>0.596</td>\n",
       "      <td>0.639647</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.704831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>prod_4</td>\n",
       "      <td>500</td>\n",
       "      <td>0.648</td>\n",
       "      <td>0.640278</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.648</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.698320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>prod_5</td>\n",
       "      <td>500</td>\n",
       "      <td>0.624</td>\n",
       "      <td>0.646013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.376</td>\n",
       "      <td>0.746326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_test = df.loc[df['dataset'] == 'test']\n",
    "df_prod = df.loc[df['dataset'] != 'test']\n",
    "df_test.reset_index(inplace=True, drop=True)\n",
    "display(HTML(df_test.to_html()))\n",
    "display(HTML(df_prod.head().to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1f3a92d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>pp fit size</th>\n",
       "      <th>svc accuracy</th>\n",
       "      <th>pp accuracy</th>\n",
       "      <th>TH</th>\n",
       "      <th>TL</th>\n",
       "      <th>FH</th>\n",
       "      <th>FL</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test</td>\n",
       "      <td>500</td>\n",
       "      <td>62.60%</td>\n",
       "      <td>61.88%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>62.60%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>37.40%</td>\n",
       "      <td>90.72%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test</td>\n",
       "      <td>1000</td>\n",
       "      <td>62.60%</td>\n",
       "      <td>62.29%</td>\n",
       "      <td>2.80%</td>\n",
       "      <td>59.80%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>37.40%</td>\n",
       "      <td>85.56%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test</td>\n",
       "      <td>2000</td>\n",
       "      <td>60.70%</td>\n",
       "      <td>62.06%</td>\n",
       "      <td>13.25%</td>\n",
       "      <td>47.45%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>39.30%</td>\n",
       "      <td>95.58%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test</td>\n",
       "      <td>4000</td>\n",
       "      <td>62.15%</td>\n",
       "      <td>62.41%</td>\n",
       "      <td>16.28%</td>\n",
       "      <td>45.88%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>37.85%</td>\n",
       "      <td>95.73%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test</td>\n",
       "      <td>8000</td>\n",
       "      <td>62.35%</td>\n",
       "      <td>62.40%</td>\n",
       "      <td>19.93%</td>\n",
       "      <td>42.43%</td>\n",
       "      <td>1.18%</td>\n",
       "      <td>36.48%</td>\n",
       "      <td>81.67%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>test</td>\n",
       "      <td>16000</td>\n",
       "      <td>62.38%</td>\n",
       "      <td>63.11%</td>\n",
       "      <td>19.39%</td>\n",
       "      <td>42.98%</td>\n",
       "      <td>1.46%</td>\n",
       "      <td>36.17%</td>\n",
       "      <td>80.54%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pp fit size</th>\n",
       "      <th>svc accuracy</th>\n",
       "      <th>pp accuracy</th>\n",
       "      <th>TH</th>\n",
       "      <th>TL</th>\n",
       "      <th>FH</th>\n",
       "      <th>FL</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>500</td>\n",
       "      <td>62.35%</td>\n",
       "      <td>63.96%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>62.35%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>37.65%</td>\n",
       "      <td>73.37%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000</td>\n",
       "      <td>62.25%</td>\n",
       "      <td>62.53%</td>\n",
       "      <td>3.21%</td>\n",
       "      <td>59.03%</td>\n",
       "      <td>0.23%</td>\n",
       "      <td>37.52%</td>\n",
       "      <td>77.07%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000</td>\n",
       "      <td>62.51%</td>\n",
       "      <td>60.33%</td>\n",
       "      <td>0.57%</td>\n",
       "      <td>61.94%</td>\n",
       "      <td>0.04%</td>\n",
       "      <td>37.45%</td>\n",
       "      <td>78.77%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4000</td>\n",
       "      <td>62.35%</td>\n",
       "      <td>61.65%</td>\n",
       "      <td>0.79%</td>\n",
       "      <td>61.55%</td>\n",
       "      <td>0.03%</td>\n",
       "      <td>37.62%</td>\n",
       "      <td>79.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8000</td>\n",
       "      <td>62.52%</td>\n",
       "      <td>62.19%</td>\n",
       "      <td>19.51%</td>\n",
       "      <td>43.01%</td>\n",
       "      <td>1.41%</td>\n",
       "      <td>36.06%</td>\n",
       "      <td>79.89%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16000</td>\n",
       "      <td>62.49%</td>\n",
       "      <td>62.87%</td>\n",
       "      <td>19.08%</td>\n",
       "      <td>43.41%</td>\n",
       "      <td>1.39%</td>\n",
       "      <td>36.11%</td>\n",
       "      <td>79.78%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pp fit size</th>\n",
       "      <th>dataset</th>\n",
       "      <th>svc accuracy</th>\n",
       "      <th>pp accuracy</th>\n",
       "      <th>TH</th>\n",
       "      <th>TL</th>\n",
       "      <th>FH</th>\n",
       "      <th>FL</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>500</td>\n",
       "      <td>prod_1</td>\n",
       "      <td>56.67%</td>\n",
       "      <td>63.13%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>56.67%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>33.20%</td>\n",
       "      <td>68.92%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000</td>\n",
       "      <td>prod_1</td>\n",
       "      <td>56.67%</td>\n",
       "      <td>60.22%</td>\n",
       "      <td>2.60%</td>\n",
       "      <td>53.33%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>34.30%</td>\n",
       "      <td>72.53%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000</td>\n",
       "      <td>prod_1</td>\n",
       "      <td>61.05%</td>\n",
       "      <td>59.92%</td>\n",
       "      <td>0.40%</td>\n",
       "      <td>60.35%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>35.80%</td>\n",
       "      <td>76.67%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4000</td>\n",
       "      <td>prod_1</td>\n",
       "      <td>61.25%</td>\n",
       "      <td>61.44%</td>\n",
       "      <td>0.60%</td>\n",
       "      <td>60.36%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>36.48%</td>\n",
       "      <td>78.06%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8000</td>\n",
       "      <td>prod_1</td>\n",
       "      <td>62.18%</td>\n",
       "      <td>61.94%</td>\n",
       "      <td>19.20%</td>\n",
       "      <td>42.55%</td>\n",
       "      <td>1.21%</td>\n",
       "      <td>35.43%</td>\n",
       "      <td>79.38%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16000</td>\n",
       "      <td>prod_1</td>\n",
       "      <td>62.49%</td>\n",
       "      <td>62.87%</td>\n",
       "      <td>19.08%</td>\n",
       "      <td>43.41%</td>\n",
       "      <td>1.39%</td>\n",
       "      <td>36.11%</td>\n",
       "      <td>79.78%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pp fit size</th>\n",
       "      <th>dataset</th>\n",
       "      <th>svc accuracy</th>\n",
       "      <th>pp accuracy</th>\n",
       "      <th>TH</th>\n",
       "      <th>TL</th>\n",
       "      <th>FH</th>\n",
       "      <th>FL</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>500</td>\n",
       "      <td>prod_9</td>\n",
       "      <td>66.80%</td>\n",
       "      <td>64.94%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>66.80%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>43.33%</td>\n",
       "      <td>77.88%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000</td>\n",
       "      <td>prod_9</td>\n",
       "      <td>65.50%</td>\n",
       "      <td>63.95%</td>\n",
       "      <td>4.10%</td>\n",
       "      <td>61.40%</td>\n",
       "      <td>0.60%</td>\n",
       "      <td>43.33%</td>\n",
       "      <td>80.15%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000</td>\n",
       "      <td>prod_9</td>\n",
       "      <td>64.15%</td>\n",
       "      <td>61.23%</td>\n",
       "      <td>0.80%</td>\n",
       "      <td>63.55%</td>\n",
       "      <td>0.10%</td>\n",
       "      <td>38.85%</td>\n",
       "      <td>80.70%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4000</td>\n",
       "      <td>prod_7</td>\n",
       "      <td>63.48%</td>\n",
       "      <td>61.90%</td>\n",
       "      <td>0.90%</td>\n",
       "      <td>62.70%</td>\n",
       "      <td>0.09%</td>\n",
       "      <td>38.66%</td>\n",
       "      <td>79.66%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8000</td>\n",
       "      <td>prod_3</td>\n",
       "      <td>62.99%</td>\n",
       "      <td>62.37%</td>\n",
       "      <td>19.85%</td>\n",
       "      <td>43.50%</td>\n",
       "      <td>1.58%</td>\n",
       "      <td>36.61%</td>\n",
       "      <td>80.30%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16000</td>\n",
       "      <td>prod_1</td>\n",
       "      <td>62.49%</td>\n",
       "      <td>62.87%</td>\n",
       "      <td>19.08%</td>\n",
       "      <td>43.41%</td>\n",
       "      <td>1.39%</td>\n",
       "      <td>36.11%</td>\n",
       "      <td>79.78%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display the\n",
    "# - test pp prediction\n",
    "# - multiple prod pp predictions (mean, min & max)\n",
    "pct_fmt = '{:,.2%}'.format\n",
    "fmt = {col: pct_fmt for col in ['svc accuracy', 'pp accuracy', 'TH', 'TL', 'FH', 'FL', 'AUC']}\n",
    "\n",
    "display(HTML(df_test.to_html(formatters=fmt)))\n",
    "\n",
    "df_mean = df_prod.groupby(['pp fit size']).mean()\n",
    "df_mean.reset_index(inplace=True)\n",
    "display(HTML(df_mean.to_html(formatters=fmt)))\n",
    "\n",
    "df_min = df_prod.groupby(['pp fit size']).min()\n",
    "df_min.reset_index(inplace=True)\n",
    "display(HTML(df_min.to_html(formatters=fmt)))\n",
    "\n",
    "\n",
    "df_max = df_prod.groupby(['pp fit size']).max()\n",
    "df_max.reset_index(inplace=True)\n",
    "display(HTML(df_max.to_html(formatters=fmt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6f886f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
