{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0acb7cc5",
   "metadata": {},
   "source": [
    "# Short text predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757faaca",
   "metadata": {},
   "source": [
    "In this notebook, we will do the following \n",
    "1. Use a simple data set (ATIS) https://www.kaggle.com/hassanamin/atis-airlinetravelinformationsystem \n",
    "2. Create three types of splits. Every split will create three types of data - training data, testing data and productiond data\n",
    "    - Random split\n",
    "    - Length based split \n",
    "    - Confidence based split\n",
    "\n",
    "Length and confidence based split are used to introduce some drift\n",
    "\n",
    "3. Train an MLPClassifier model on the train data. Verify on test data and also look at the performance of the model on the production data\n",
    "4. Fit a short text predictor using the trained model, train/test data\n",
    "5. Calculate the predictions as returned by the predictor on the production data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4faa229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/anupamamurthi/Documents/GitHub/UQ360/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e2c6cb",
   "metadata": {},
   "source": [
    "# Performance Predictor on ATIS dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64cf7d3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>example</th>\n",
       "      <th>intent</th>\n",
       "      <th>iob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>i want to fly from boston at 838 am and arrive...</td>\n",
       "      <td>atis_flight</td>\n",
       "      <td>O O O O O O B-fromloc.city_name O B-depart_tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>what flights are available from pittsburgh to ...</td>\n",
       "      <td>atis_flight</td>\n",
       "      <td>O O O O O O B-fromloc.city_name O B-toloc.city...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>what is the arrival time in san francisco for ...</td>\n",
       "      <td>atis_flight_time</td>\n",
       "      <td>O O O O B-flight_time I-flight_time O B-fromlo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>cheapest airfare from tacoma to orlando</td>\n",
       "      <td>atis_airfare</td>\n",
       "      <td>O B-cost_relative O O B-fromloc.city_name O B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>round trip fares from pittsburgh to philadelph...</td>\n",
       "      <td>atis_airfare</td>\n",
       "      <td>O B-round_trip I-round_trip O O B-fromloc.city...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            example  \\\n",
       "0           0  i want to fly from boston at 838 am and arrive...   \n",
       "1           1  what flights are available from pittsburgh to ...   \n",
       "2           2  what is the arrival time in san francisco for ...   \n",
       "3           3            cheapest airfare from tacoma to orlando   \n",
       "4           4  round trip fares from pittsburgh to philadelph...   \n",
       "\n",
       "             intent                                                iob  \n",
       "0       atis_flight  O O O O O O B-fromloc.city_name O B-depart_tim...  \n",
       "1       atis_flight  O O O O O O B-fromloc.city_name O B-toloc.city...  \n",
       "2  atis_flight_time  O O O O B-flight_time I-flight_time O B-fromlo...  \n",
       "3      atis_airfare  O B-cost_relative O O B-fromloc.city_name O B-...  \n",
       "4      atis_airfare  O B-round_trip I-round_trip O O B-fromloc.city...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load ATIS raw data. A pointer to the data can also be found here: https://www.kaggle.com/hassanamin/atis-airlinetravelinformationsystem/version/1\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "li_data = []\n",
    "li_labels = []\n",
    "li_len = []\n",
    "\n",
    "df = pd.read_csv(\n",
    "    '/Users/anupamamurthi/Documents/GitHub/UQ360/data/text/atis/atis.train.w-intent.iob.csv', index_col=None, header=0)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aebacff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\n",
      "['i want to fly from boston at 838 am and arrive in denver at 1110 in the morning'\n",
      " 'what flights are available from pittsburgh to baltimore on thursday morning'\n",
      " 'what is the arrival time in san francisco for the 755 am flight leaving washington'\n",
      " 'cheapest airfare from tacoma to orlando'\n",
      " 'round trip fares from pittsburgh to philadelphia under 1000 dollars'\n",
      " 'i need a flight tomorrow from columbus to minneapolis'\n",
      " 'what kind of aircraft is used on a flight from cleveland to dallas'\n",
      " 'show me the flights from pittsburgh to los angeles on thursday'\n",
      " 'all flights from boston to washington'\n",
      " 'what kind of ground transportation is available in denver']\n",
      "Labels data\n",
      "['atis_flight' 'atis_flight' 'atis_flight_time' 'atis_airfare'\n",
      " 'atis_airfare' 'atis_flight' 'atis_aircraft' 'atis_flight' 'atis_flight'\n",
      " 'atis_ground_service']\n"
     ]
    }
   ],
   "source": [
    "# create training data and labels\n",
    "li_data.append(df['example'])\n",
    "li_labels.append(df['intent']) \n",
    "\n",
    "frame = pd.concat(li_data, axis=0, ignore_index=True)\n",
    "npdata = frame.to_numpy()\n",
    "\n",
    "frame_labels = pd.concat(li_labels, axis=0, ignore_index=True)\n",
    "npdata_labels = frame_labels.to_numpy()\n",
    " \n",
    "print(\"Data\")\n",
    "print(npdata[:10])\n",
    "\n",
    "print(\"Labels data\")\n",
    "print(npdata_labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f82bf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple implementation to fit and transform text data\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "\n",
    "class UseTransformer(TransformerMixin):\n",
    "    '''\n",
    "    Wrapper to run the Universal Sentence Embeddings (USE) encoder.\n",
    "    Organizes the USE into the fit, transform and fit_transform standard methods of TransformerMixin.\n",
    "    '''\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        encoder = hub.load('https://tfhub.dev/google/universal-sentence-encoder/4')\n",
    "        return encoder(X.ravel()).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff561235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(x, y):\n",
    "    \"\"\"\n",
    "    returns model object\n",
    "    \"\"\"\n",
    "    from sklearn.neural_network import MLPClassifier \n",
    "    model = MLPClassifier()\n",
    "    model.fit(x, y)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3725e357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_svm(x, y):\n",
    "    \"\"\"\n",
    "    returns model object\n",
    "    \"\"\"\n",
    "    from sklearn.svm import SVC\n",
    "    model = SVC(probability=True)\n",
    "    model.fit(x, y)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc817b8",
   "metadata": {},
   "source": [
    "# Use Case: Random Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fc23c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train test prod data\n",
    "\n",
    "def create_train_test_prod_split(x, y, test_size=0.25 ):\n",
    "    \"\"\"\n",
    "    returns x_train, y_train, x_test, y_test, x_prod, y_prod\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y,\n",
    "                                                                    test_size=0.25, \n",
    "                                                                random_state=42)\n",
    "\n",
    "    x_test, x_prod, y_test, y_prod = train_test_split(x_test, y_test,\n",
    "                                                                    test_size=0.25, \n",
    "                                                                random_state=42)\n",
    "\n",
    "    print(x_train.shape, y_train.shape, x_test.shape, y_test.shape, x_prod.shape, y_prod.shape)\n",
    "\n",
    "    \n",
    "    print(\"Training data size:\", x_train.shape)\n",
    "    print(\"Test data size:\", x_test.shape)\n",
    "    print(\"Prod data size:\", x_prod.shape)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test, x_prod, y_prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3e8388b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data points (4952,)\n",
      "(3714,) (3714,) (928,) (928,) (310,) (310,)\n",
      "Training data size: (3714,)\n",
      "Test data size: (928,)\n",
      "Prod data size: (310,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Total data points\", npdata.shape)\n",
    "x_train, y_train, x_test, y_test, x_prod, y_prod = create_train_test_prod_split(npdata, npdata_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f76881ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data before encoding (3714,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-03 22:59:57.545092: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-10-03 22:59:59.511597: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data after encoding (3714, 512)\n"
     ]
    }
   ],
   "source": [
    "# Fit a basic SVM classifier after encoding\n",
    "\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "obj = UseTransformer()\n",
    "print(\"Training data before encoding\", x_train.shape)\n",
    "x_train_encoded = obj.transform(X=x_train)\n",
    "print(\"Training data after encoding\", x_train_encoded.shape)\n",
    "model = train_model_svm(x_train_encoded, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b314f8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data after encoding (928, 512)\n",
      "Prod data after encoding (310, 512)\n",
      "acc on test 0.9633620689655172\n",
      "acc on prod 0.9290322580645162\n"
     ]
    }
   ],
   "source": [
    "x_test_encoded = obj.transform(X=x_test)\n",
    "print(\"Test data after encoding\", x_test_encoded.shape)\n",
    "\n",
    "x_prod_encoded = obj.transform(X=x_prod)\n",
    "print(\"Prod data after encoding\", x_prod_encoded.shape)\n",
    "\n",
    "# acc on test data\n",
    "acc = model.score(x_test_encoded, y_test)\n",
    "print(\"acc on test\", acc)\n",
    "\n",
    "\n",
    "# acc on prod data\n",
    "score = model.score(x_prod_encoded, y_prod)\n",
    "print(\"acc on prod\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c6ce3f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(probability=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eab80161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch features : None\n",
      "Pointwise features : ['confidence_top', 'confidence_delta', 'confidence_entropy', 'gbm', 'class_frequency', 'mlp', 'svc']\n",
      "Blackbox features : None\n",
      "Predictor type : text_ensemble\n",
      "Features extracted for : odict_keys(['confidence_top', 'confidence_delta', 'confidence_entropy', 'gbm_1', 'gbm_2', 'class_frequency', 'mlp_1', 'mlp_2', 'svc_1', 'svc_2'])\n"
     ]
    }
   ],
   "source": [
    "# It is possible to train the predictor using encoded data or using raw text. \n",
    "\n",
    "# In the below example, we are using raw text to train the predictor but x_train can be swapped with x_train_encoded\n",
    "\n",
    "from uq360.algorithms.blackbox_metamodel.short_text_classification import ShortTextClassificationWrapper\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "p = ShortTextClassificationWrapper(base_model=model)\n",
    "\n",
    "p.fit(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2384617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incoming data contains raw text.\n",
      "Using an encoder.... %s <uq360.utils.utils.UseTransformer object at 0x19fb71c40>\n",
      "Shapes before encoding %s (310,)\n",
      "Shapes after encoding %s (310, 512)\n",
      "Features extracted for : odict_keys(['confidence_top', 'confidence_delta', 'confidence_entropy', 'gbm_1', 'gbm_2', 'class_frequency', 'mlp_1', 'mlp_2', 'svc_1', 'svc_2'])\n"
     ]
    }
   ],
   "source": [
    "# Check the predictions on prod data\n",
    "\n",
    "# x_prod is raw text. Predicting using raw text. Encoding happens inside the Predictor\n",
    "\n",
    "out, y_pred, y_score = p.predict(x_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ee58c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92.146829810901"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63739ae0",
   "metadata": {},
   "source": [
    "# Use Case: Length based split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e453b14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = np.concatenate((x_train, x_test, x_prod), axis=0)\n",
    "y_1 = np.concatenate((y_train, y_test, y_prod), axis =0)\n",
    "\n",
    "base_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5bb3a263",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# calculate length of every \"intent\"\n",
    "len_of_x_1 = np.asarray([len(i) for i in x_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48fc3210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the median\n",
    "median = np.median(len_of_x_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc1dbe02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two buckets - one bucket holds all the indices that are less than median length and the other bucket holds all the indices > median legth\n",
    "len_less_than_median = np.where(len_of_x_1 < median)\n",
    "len_greater_than_median = np.where(len_of_x_1 >= median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5c2faca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples 4952\n",
      "Train test samples 2476.0\n",
      "Prod samples 2476.0\n"
     ]
    }
   ],
   "source": [
    "# training/test data -> 70 pct from less_than_median and 30 pct from greater_than_median\n",
    "\n",
    "total_samples = x_1.shape[0]\n",
    "train_test_samples = x_1.shape[0] * 0.5\n",
    "prod_samples = train_test_samples\n",
    "\n",
    "print(\"Total samples\", total_samples)\n",
    "print(\"Train test samples\", train_test_samples)\n",
    "print(\"Prod samples\", prod_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ce5e043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data before encoding (4952,)\n",
      "Training data after encoding (4952, 512)\n"
     ]
    }
   ],
   "source": [
    "# Fit a basic classifier after encoding\n",
    "\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "obj = UseTransformer()\n",
    "print(\"Training data before encoding\", x_1.shape)\n",
    "x_1_train_encoded = obj.transform(X=x_1)\n",
    "print(\"Training data after encoding\", x_1_train_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9968d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(x, y, bucket_1_indices, bucket_2_indices, split_ratio=0.3, test_size = 0.25): \n",
    "    \"\"\"\n",
    "    returns: x_train, y_train, x_test, y_test, x_prod, y_prod\n",
    "    \"\"\"\n",
    "    train_test_samples = x.shape[0] * 0.5\n",
    "    training_test_data_from_bucket_1 = np.random.choice(bucket_1_indices[0],  int(train_test_samples *split_ratio), replace=False )\n",
    "    training_test_data_from_bucket_2 = np.random.choice(bucket_2_indices[0],  int(train_test_samples *(1-split_ratio)), replace=False )\n",
    "    \n",
    "    prod_data_from_bucket_1 = np.setdiff1d (bucket_1_indices, training_test_data_from_bucket_1)\n",
    "    prod_data_from_bucket_2 = np.setdiff1d (bucket_2_indices, training_test_data_from_bucket_2)\n",
    "\n",
    "    training_test_data_indices = np.concatenate((training_test_data_from_bucket_1, training_test_data_from_bucket_2), axis=0)\n",
    "    prod_indices = np.concatenate((prod_data_from_bucket_1, prod_data_from_bucket_2), axis=0)\n",
    "\n",
    "    training_test_data = x[training_test_data_indices]\n",
    "    training_test_label = y[training_test_data_indices]\n",
    "\n",
    "    prod_test_data = x[prod_indices]\n",
    "    prod_test_label = y[prod_indices]\n",
    "\n",
    "\n",
    "    from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "    x_train_new, x_test_new, y_train_new, y_test_new = train_test_split(training_test_data, training_test_label,\n",
    "                                                                    test_size=test_size, \n",
    "                                                                random_state=42)\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "    print(\"Training data size:\", x_train_new.shape)\n",
    "    print(\"Test data size:\", x_test_new.shape)\n",
    "    print(\"Prod data size:\", prod_test_data.shape)\n",
    "    \n",
    "    return x_train_new, y_train_new, x_test_new, y_test_new, prod_test_data, prod_test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72b1c0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_svm(x, y):\n",
    "    \"\"\"\n",
    "    returns model object\n",
    "    \"\"\"\n",
    "    from sklearn.svm import SVC\n",
    "    model = SVC(probability=True)\n",
    "    model.fit(x, y)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d1139462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: (1856, 512)\n",
      "Test data size: (619, 512)\n",
      "Prod data size: (2477, 512)\n"
     ]
    }
   ],
   "source": [
    "# split\n",
    "x_1_train, y_1_train, x_1_test, y_1_test, x_1_prod, y_1_prod = split(x_1_train_encoded, y_1,len_less_than_median, len_greater_than_median, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f455435a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test 0.9192245557350566\n",
      "accuracy on prod 0.9236980218005651\n"
     ]
    }
   ],
   "source": [
    "# train a model\n",
    "model_train_on_length_based_split = train_model_svm(x_1_train, y_1_train)\n",
    "\n",
    "# check accuracy on test and prod set\n",
    "print(\"accuracy on test\", model_train_on_length_based_split.score(x_1_test, y_1_test))\n",
    "print(\"accuracy on prod\", model_train_on_length_based_split.score(x_1_prod, y_1_prod))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3beb0088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch features : None\n",
      "Pointwise features : ['confidence_top', 'confidence_delta', 'confidence_entropy', 'gbm', 'class_frequency', 'mlp', 'svc']\n",
      "Blackbox features : None\n",
      "Predictor type : text_ensemble\n",
      "Features extracted for : odict_keys(['confidence_top', 'confidence_delta', 'confidence_entropy', 'gbm_1', 'gbm_2', 'class_frequency', 'mlp_1', 'mlp_2', 'svc_1', 'svc_2'])\n",
      "Incoming data is already encoded\n",
      "Features extracted for : odict_keys(['confidence_top', 'confidence_delta', 'confidence_entropy', 'gbm_1', 'gbm_2', 'class_frequency', 'mlp_1', 'mlp_2', 'svc_1', 'svc_2'])\n",
      "87.47826237948264\n"
     ]
    }
   ],
   "source": [
    "# fit a predictor\n",
    "\n",
    "# from performance_predictors import Predictor\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from uq360.algorithms.blackbox_metamodel.short_text_classification import ShortTextClassificationWrapper\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "p2 = ShortTextClassificationWrapper(base_model=model_train_on_length_based_split)\n",
    "\n",
    "p2.fit(x_1_train, y_1_train, x_1_test, y_1_test)\n",
    "\n",
    "pred, y_pred, y_score = p2.predict(x_1_prod)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e395ab32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted accuracy 87.47826237948264\n"
     ]
    }
   ],
   "source": [
    "print(\"predicted accuracy\", pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0382d1",
   "metadata": {},
   "source": [
    "# Use Case: Median Confidence based split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2677677f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab x and y by concatenating train, test, prod data\n",
    "x = np.concatenate((x_train_encoded, x_test_encoded, x_prod_encoded), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "97d9a063",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y = np.concatenate((y_train, y_test, y_prod), axis =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b9c5e4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = model\n",
    "\n",
    "# use the base model and grab the top confidence for every data point that we have\n",
    "\n",
    "x_proba = base_model.predict_proba(x)\n",
    "confs_sorted = np.sort(x_proba) \n",
    "top_confs = confs_sorted[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "024800da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the median\n",
    "median  = np.median(top_confs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8ce81a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two buckets\n",
    "less_than_median = np.where(top_confs < median)\n",
    "greater_than_median = np.where(top_confs >= median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "44ed34e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: (1856, 512)\n",
      "Test data size: (619, 512)\n",
      "Prod data size: (2477, 512)\n"
     ]
    }
   ],
   "source": [
    "# Given the two buckets, shuffle the data into train, test, prod sets\n",
    "\n",
    "x_train_new, y_train_new, x_test_new, y_test_new, prod_test_data, prod_test_label = split(x, y,less_than_median,greater_than_median, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3f9450a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc on test 0.9499192245557351\n",
      "acc on prod 0.846992329430763\n"
     ]
    }
   ],
   "source": [
    "# train a new model using the training data created in the previous step\n",
    "model_trained_on_conf_based_split = train_model_svm(x_train_new, y_train_new)\n",
    "# acc on test data\n",
    "acc = model_trained_on_conf_based_split.score(x_test_new, y_test_new)\n",
    "print(\"acc on test\", acc)\n",
    "\n",
    "# acc on prod data\n",
    "acc = model_trained_on_conf_based_split.score(prod_test_data, prod_test_label)\n",
    "print(\"acc on prod\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1b36089d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch features : None\n",
      "Pointwise features : ['confidence_top', 'confidence_delta', 'confidence_entropy', 'gbm', 'class_frequency', 'mlp', 'svc']\n",
      "Blackbox features : None\n",
      "Predictor type : text_ensemble\n",
      "Features extracted for : odict_keys(['confidence_top', 'confidence_delta', 'confidence_entropy', 'gbm_1', 'gbm_2', 'class_frequency', 'mlp_1', 'mlp_2', 'svc_1', 'svc_2'])\n",
      "Incoming data is already encoded\n",
      "Features extracted for : odict_keys(['confidence_top', 'confidence_delta', 'confidence_entropy', 'gbm_1', 'gbm_2', 'class_frequency', 'mlp_1', 'mlp_2', 'svc_1', 'svc_2'])\n"
     ]
    }
   ],
   "source": [
    "# train a performance predictor\n",
    "\n",
    "from uq360.algorithms.blackbox_metamodel.short_text_classification import ShortTextClassificationWrapper\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "p1 = ShortTextClassificationWrapper(base_model=model_trained_on_conf_based_split)\n",
    "\n",
    "p1.fit(x_train_new, y_train_new, x_test_new, y_test_new)\n",
    "\n",
    "pred, y_pred, y_score = p1.predict(prod_test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8c4e7021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted accuracy 79.11417297198746\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted accuracy\" , pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dba3b26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
